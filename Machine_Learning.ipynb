{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Library**"
      ],
      "metadata": {
        "id": "4yLTYVfX_T9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uw2lq6rNfkM_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, explained_variance_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from io import StringIO\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "cKJqHRKNEqDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv(\"Copy of Rotten_Tomatoes.csv\")"
      ],
      "metadata": {
        "id": "fuyJSj8-AHY7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display of Data Info & Describe**"
      ],
      "metadata": {
        "id": "_crF7WK-EwAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Display of data.info()\n",
        "print(\"=== Data Info ===\")\n",
        "buffer = StringIO()  # Create a file-like buffer\n",
        "data.info(buf=buffer)\n",
        "info_str = buffer.getvalue()  # Retrieve the string content\n",
        "print(info_str)\n",
        "\n",
        "# Pretty display of data.describe()\n",
        "print(\"\\n=== Data Description ===\")\n",
        "describe_table = data.describe().reset_index()\n",
        "print(tabulate(describe_table, headers='keys', tablefmt='pretty', showindex=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVl9k8gTAJiS",
        "outputId": "38fce9bf-00f6-42b4-c6e8-a2dbc7d35840"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Data Info ===\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16638 entries, 0 to 16637\n",
            "Data columns (total 16 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   movie_title         16638 non-null  object \n",
            " 1   movie_info          16614 non-null  object \n",
            " 2   critics_consensus   8309 non-null   object \n",
            " 3   rating              16638 non-null  object \n",
            " 4   genre               16621 non-null  object \n",
            " 5   directors           16524 non-null  object \n",
            " 6   writers             15289 non-null  object \n",
            " 7   cast                16354 non-null  object \n",
            " 8   in_theaters_date    15823 non-null  object \n",
            " 9   on_streaming_date   16636 non-null  object \n",
            " 10  runtime_in_minutes  16483 non-null  float64\n",
            " 11  studio_name         16222 non-null  object \n",
            " 12  tomatometer_status  16638 non-null  object \n",
            " 13  tomatometer_rating  16638 non-null  int64  \n",
            " 14  tomatometer_count   16638 non-null  int64  \n",
            " 15  audience_rating     16386 non-null  float64\n",
            "dtypes: float64(2), int64(2), object(12)\n",
            "memory usage: 2.0+ MB\n",
            "\n",
            "\n",
            "=== Data Description ===\n",
            "+-------+--------------------+--------------------+--------------------+--------------------+\n",
            "| index | runtime_in_minutes | tomatometer_rating | tomatometer_count  |  audience_rating   |\n",
            "+-------+--------------------+--------------------+--------------------+--------------------+\n",
            "| count |      16483.0       |      16638.0       |      16638.0       |      16386.0       |\n",
            "| mean  | 102.39149426682036 | 60.466522418559926 | 56.607104219257124 | 60.47082875625534  |\n",
            "|  std  | 25.02801086709842  | 28.587230017631583 | 66.38379951576111  | 20.462367575517256 |\n",
            "|  min  |        1.0         |        0.0         |        5.0         |        0.0         |\n",
            "|  25%  |        90.0        |        38.0        |        12.0        |        45.0        |\n",
            "|  50%  |        99.0        |        66.0        |        28.0        |        62.0        |\n",
            "|  75%  |       111.0        |        86.0        |        76.0        |        77.0        |\n",
            "|  max  |       2000.0       |       100.0        |       497.0        |       100.0        |\n",
            "+-------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle missing values**"
      ],
      "metadata": {
        "id": "iVtre8nNGVYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "data = data.dropna()  # Simplified handling"
      ],
      "metadata": {
        "id": "bstFEMceCJ3B"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Feature selection**\n",
        "- **Identify categorical and numerical columns**\n",
        "- **Preprocessing**"
      ],
      "metadata": {
        "id": "oabGGxD_I5Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "X = data.drop(columns=['audience_rating'])\n",
        "y = data['audience_rating']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['genre', 'directors', 'writers', 'studio_name']\n",
        "numerical_cols = ['runtime_in_minutes', 'tomatometer_rating']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_cols),  # Scale numerical features\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # OneHotEncode categorical features\n",
        "])"
      ],
      "metadata": {
        "id": "Bct4HOrxCNid"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define models**"
      ],
      "metadata": {
        "id": "ZnqM9tWnYmHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestRegressor(),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Lasso Regression\": Lasso(),\n",
        "    \"Support Vector Regressor\": SVR()\n",
        "}"
      ],
      "metadata": {
        "id": "q3IU1chxhH5C"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Train-Test Split**\n",
        "- **Train model**\n",
        "- **Predict and evaluate**"
      ],
      "metadata": {
        "id": "OXuW5WDrRSIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate OSM summary\n",
        "def calculate_osm(y_true, y_pred):\n",
        "    return {\n",
        "        \"R² Score\": r2_score(y_true, y_pred),\n",
        "        \"RMSE\": mean_squared_error(y_true, y_pred, squared=False),\n",
        "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
        "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
        "        \"Explained Variance\": explained_variance_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Split the data into training and test sets (optional but recommended for better evaluation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Create pipeline for each model with preprocessing step\n",
        "    model_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "        ('model', model)  # Apply model\n",
        "    ])\n",
        "    model_pipeline.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "    y_pred = model_pipeline.predict(X_test)  # Predict on the test data\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    osm_summary = calculate_osm(y_test, y_pred)  # Use test data for evaluation\n",
        "    osm_summary[\"Model\"] = model_name  # Add model name for clarity\n",
        "    results.append(osm_summary)\n",
        "\n",
        "# Pretty print results in a table format\n",
        "print(\"\\n=== Model Performance Comparison (OSM) ===\")\n",
        "print(tabulate(results, headers=\"keys\", tablefmt=\"pretty\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMefE4qfCTvS",
        "outputId": "4c967c0f-34ff-4084-8782-1051ef2fee21"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Performance Comparison (OSM) ===\n",
            "+----------------------+--------------------+--------------------+--------------------+----------------------+--------------------------+\n",
            "|       R² Score       |        RMSE        |        MSE         |        MAE         |  Explained Variance  |          Model           |\n",
            "+----------------------+--------------------+--------------------+--------------------+----------------------+--------------------------+\n",
            "|  0.5399032148900288  | 13.229661034113064 | 175.02393107752957 | 10.220519053876478 |  0.5399032471286811  |      Random Forest       |\n",
            "|  0.5680315132334677  | 12.81888201092687  | 164.32373601006452 | 10.271470519191245 |  0.5680563928909064  |    Gradient Boosting     |\n",
            "| -0.34930719502721064 | 22.655806967609344 | 513.2855893535761  | 16.379138251761265 | -0.34921488265097644 |    Linear Regression     |\n",
            "|  0.5262224456864176  | 13.424909246494215 | 180.22818827660586 | 10.834126100932135 |  0.5262814237097795  |     Lasso Regression     |\n",
            "| 0.49064392502177534  | 13.919859853445526 | 193.76249833956453 | 10.600624724575018 |  0.4943837980921141  | Support Vector Regressor |\n",
            "+----------------------+--------------------+--------------------+--------------------+----------------------+--------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- R² Score: Measures how well the model explains the variability of the target\n",
        "ariable. Closer to 1 is better.\n",
        "- RMSE (Root Mean Squared Error): Standard deviation of the prediction errors. Lower is better.\n",
        "- MSE (Mean Squared Error): Average of the squared errors. Lower is better.\n",
        "- MAE (Mean Absolute Error): Average absolute difference between predictions and actual values. Lower is better.\n",
        "- Explained Variance: Indicates how much variance the model captures. Closer to 1 is better."
      ],
      "metadata": {
        "id": "19dFYRF-WuQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics:\n",
        "**R² Score (Coefficient of Determination):**\n",
        "\n",
        "- This score indicates how well the model’s predictions match the actual data.\n",
        "Range: 0 to 1 (higher is better)\n",
        "- 1 means the model perfectly fits the data.\n",
        "- 0 means the model does not explain the variance of the target at all.\n",
        "- Negative values could indicate a model that is worse than a simple mean-based model.\n",
        "\n",
        "**RMSE (Root Mean Squared Error):**\n",
        "\n",
        "- This measures the square root of the average squared differences between predicted and actual values.\n",
        "- Lower is better.\n",
        "- It gives an indication of how far off, on average, your predictions are from the actual values.\n",
        "\n",
        "**MSE (Mean Squared Error):**\n",
        "\n",
        "- This measures the average of the squared differences between predicted and actual values.\n",
        "- Lower is better.\n",
        "\n",
        "**MAE (Mean Absolute Error):**\n",
        "\n",
        "- This calculates the average of the absolute differences between predicted and actual values.\n",
        "- Lower is better.\n",
        "- This gives a sense of how large the errors are in your predictions, without considering their direction (whether the prediction is higher or lower than the actual value).\n",
        "\n",
        "**Explained Variance:**\n",
        "\n",
        "- This shows how much of the variance in the target variable can be explained by the model.\n",
        "- Higher is better.\n",
        "If it is close to 1, the model is explaining most of the variance."
      ],
      "metadata": {
        "id": "lPkMxShXiCnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Determine if the Output is Correct:\n",
        "**Comparing R² scores:**\n",
        "\n",
        "- A higher R² score means better predictive accuracy. If a model’s R² score is\n",
        "significantly higher than others, it is performing better.\n",
        "\n",
        "**Evaluating RMSE, MSE, MAE:**\n",
        "\n",
        "- Check which model has the lowest error values (RMSE, MSE, and MAE). Models with lower values for these metrics are making more accurate predictions.\n",
        "- For example, a model with an RMSE of 1.24 is performing better than one with an RMSE of 1.76, so the first model is preferable.\n",
        "\n",
        "**Explained Variance:**\n",
        "\n",
        "- This should also be high for better models. If your model's explained variance is around 0.8 or higher, it is explaining a good portion of the target's variance.\n",
        "\n",
        "**Conclusion:**\n",
        "- The output is correct if you see reasonable values for these metrics, and if the models are performing similarly or better than simpler models (e.g., a random guess or the mean of the target variable).\n",
        "- If your R² score is very low or negative, or if your error metrics (RMSE, MSE, MAE) are unusually high, the model is not performing well.\n"
      ],
      "metadata": {
        "id": "wlkJFv2ujkUi"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}